{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation and Text Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the ways to data augmentation techniques to improve the results in NLP models is to translate the text into another language and then re-translate this back into English. This is called back translation. In this notebook the nlpaug package will be used: https://nlpaug.readthedocs.io/en/latest/. \n",
    "\n",
    "Originally the Python Translators library was used, but there were continuous \"Too Many Requests for url\" error messages generated as the package kept pinging Google Translate for each of the 7613 rows. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2023-09-23T04:32:56.511769Z",
     "iopub.status.busy": "2023-09-23T04:32:56.510856Z",
     "iopub.status.idle": "2023-09-23T04:33:52.256456Z",
     "shell.execute_reply": "2023-09-23T04:33:52.255052Z",
     "shell.execute_reply.started": "2023-09-23T04:32:56.511732Z"
    }
   },
   "outputs": [],
   "source": [
    "!pip install keras-core --upgrade\n",
    "!pip install -q keras-nlp --upgrade\n",
    "!pip install nlpaug\n",
    "!pip install sacremoses\n",
    "\n",
    "import os\n",
    "os.environ['KERAS_BACKEND'] = 'tensorflow'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-23T04:35:16.473941Z",
     "iopub.status.busy": "2023-09-23T04:35:16.473531Z",
     "iopub.status.idle": "2023-09-23T04:35:46.371657Z",
     "shell.execute_reply": "2023-09-23T04:35:46.370645Z",
     "shell.execute_reply.started": "2023-09-23T04:35:16.473904Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import tensorflow as tf\n",
    "import keras_core as keras\n",
    "import keras_nlp\n",
    "import nlpaug.augmenter.word as naw\n",
    "from tqdm import tqdm\n",
    "\n",
    "import random as python_random\n",
    "import re\n",
    "import string\n",
    "import emoji\n",
    "import nltk\n",
    "\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "print(\"KerasNLP version:\", keras_nlp.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-23T04:35:46.379145Z",
     "iopub.status.busy": "2023-09-23T04:35:46.376402Z",
     "iopub.status.idle": "2023-09-23T04:35:46.503174Z",
     "shell.execute_reply": "2023-09-23T04:35:46.502165Z",
     "shell.execute_reply.started": "2023-09-23T04:35:46.379107Z"
    }
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"/kaggle/input/nlp-getting-started/train.csv\")\n",
    "test = pd.read_csv(\"/kaggle/input/nlp-getting-started/test.csv\")\n",
    "\n",
    "print('Training Set Shape = {}'.format(train.shape))\n",
    "print('Training Set Memory Usage = {:.2f} MB'.format(train.memory_usage().sum() / 1024**2))\n",
    "print('Test Set Shape = {}'.format(test.shape))\n",
    "print('Test Set Memory Usage = {:.2f} MB'.format(test.memory_usage().sum() / 1024**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-23T04:35:46.510577Z",
     "iopub.status.busy": "2023-09-23T04:35:46.507873Z",
     "iopub.status.idle": "2023-09-23T04:35:46.540128Z",
     "shell.execute_reply": "2023-09-23T04:35:46.538976Z",
     "shell.execute_reply.started": "2023-09-23T04:35:46.510542Z"
    }
   },
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-23T04:35:46.547793Z",
     "iopub.status.busy": "2023-09-23T04:35:46.545411Z",
     "iopub.status.idle": "2023-09-23T04:35:46.579746Z",
     "shell.execute_reply": "2023-09-23T04:35:46.578393Z",
     "shell.execute_reply.started": "2023-09-23T04:35:46.547756Z"
    }
   },
   "outputs": [],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A lot of the tweets in the dataset need to be cleaned up. Doing so should improve the results. In researching a way to clean up this text, the following Stack Overflow post was extremely helpful: https://stackoverflow.com/questions/64719706/cleaning-twitter-data-pandas-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-23T04:35:46.589036Z",
     "iopub.status.busy": "2023-09-23T04:35:46.586454Z",
     "iopub.status.idle": "2023-09-23T04:35:48.173514Z",
     "shell.execute_reply": "2023-09-23T04:35:48.172308Z",
     "shell.execute_reply.started": "2023-09-23T04:35:46.588992Z"
    }
   },
   "outputs": [],
   "source": [
    "train_clean_tweets = []\n",
    "for tweet in train['text']:\n",
    "    tweet = re.sub(\"@[A-Za-z0-9]+\",\"\",tweet) #Remove @ sign\n",
    "    tweet = re.sub(r\"(?:\\@|http?\\://|https?\\://|www)\\S+\", \"\", tweet) #Remove http links\n",
    "    tweet = \" \".join(tweet.split())\n",
    "    emojis = emoji.distinct_emoji_list(tweet)\n",
    "    tweet = ''.join(c for c in tweet if c not in emojis) #Remove Emojis\n",
    "    tweet = tweet.replace(\"#\", \"\").replace(\"_\", \" \") #Remove hashtag sign but keep the text\n",
    "    #tweet = \" \".join(w for w in nltk.wordpunct_tokenize(tweet) \\\n",
    "         #if w.lower() in tweet or not w.isalpha())\n",
    "    train_clean_tweets.append(tweet)\n",
    "    \n",
    "train['clean_text'] = train_clean_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-23T04:35:48.181846Z",
     "iopub.status.busy": "2023-09-23T04:35:48.179108Z",
     "iopub.status.idle": "2023-09-23T04:35:48.210169Z",
     "shell.execute_reply": "2023-09-23T04:35:48.209097Z",
     "shell.execute_reply.started": "2023-09-23T04:35:48.18181Z"
    }
   },
   "outputs": [],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-23T04:35:48.216768Z",
     "iopub.status.busy": "2023-09-23T04:35:48.215041Z",
     "iopub.status.idle": "2023-09-23T04:35:48.966435Z",
     "shell.execute_reply": "2023-09-23T04:35:48.965401Z",
     "shell.execute_reply.started": "2023-09-23T04:35:48.21672Z"
    }
   },
   "outputs": [],
   "source": [
    "test_clean_tweets = []\n",
    "for tweet in test['text']:\n",
    "    tweet = re.sub(\"@[A-Za-z0-9]+\",\"\",tweet) #Remove @ sign\n",
    "    tweet = re.sub(r\"(?:\\@|http?\\://|https?\\://|www)\\S+\", \"\", tweet) #Remove http links\n",
    "    tweet = \" \".join(tweet.split())\n",
    "    emojis = emoji.distinct_emoji_list(tweet)\n",
    "    tweet = ''.join(c for c in tweet if c not in emojis) #Remove Emojis\n",
    "    tweet = tweet.replace(\"#\", \"\").replace(\"_\", \" \") #Remove hashtag sign but keep the text\n",
    "    #tweet = \" \".join(w for w in nltk.wordpunct_tokenize(tweet) \\\n",
    "         #if w.lower() in tweet or not w.isalpha())\n",
    "    test_clean_tweets.append(tweet)\n",
    "    \n",
    "test['clean_text'] = test_clean_tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Varying Target Values for the Same Tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the number of unique values in each column of the train dataset it shows that there are 7613 total columns, but only 6922 of the input columns are unique, which is a total of 791 rows. That is a lot. The question whether a unique input value with many occurances are all labeled with the same target values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-23T04:35:48.968227Z",
     "iopub.status.busy": "2023-09-23T04:35:48.967837Z",
     "iopub.status.idle": "2023-09-23T04:35:48.996313Z",
     "shell.execute_reply": "2023-09-23T04:35:48.995216Z",
     "shell.execute_reply.started": "2023-09-23T04:35:48.968193Z"
    }
   },
   "outputs": [],
   "source": [
    "train.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To explore this potential labeling issue, a new column called 'unique_input' is created to be able to look at some of the larger occurances of unique values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-23T04:35:48.998445Z",
     "iopub.status.busy": "2023-09-23T04:35:48.998089Z",
     "iopub.status.idle": "2023-09-23T04:35:49.007275Z",
     "shell.execute_reply": "2023-09-23T04:35:49.00623Z",
     "shell.execute_reply.started": "2023-09-23T04:35:48.998412Z"
    }
   },
   "outputs": [],
   "source": [
    "train['unique_text'] = pd.factorize(train['clean_text'])[0] + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-23T04:35:49.011976Z",
     "iopub.status.busy": "2023-09-23T04:35:49.011406Z",
     "iopub.status.idle": "2023-09-23T04:35:49.02995Z",
     "shell.execute_reply": "2023-09-23T04:35:49.029315Z",
     "shell.execute_reply.started": "2023-09-23T04:35:49.011944Z"
    }
   },
   "outputs": [],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the top five unique occurances, only the 4th one, 4061, had variations in the target values. It doesn't appear to be a disaster, but 5 out of 17 occurances were coded as a disaster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-23T04:35:49.031816Z",
     "iopub.status.busy": "2023-09-23T04:35:49.031221Z",
     "iopub.status.idle": "2023-09-23T04:35:49.045441Z",
     "shell.execute_reply": "2023-09-23T04:35:49.044518Z",
     "shell.execute_reply.started": "2023-09-23T04:35:49.031782Z"
    }
   },
   "outputs": [],
   "source": [
    "train['unique_text'].value_counts().nlargest(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-23T04:35:49.047393Z",
     "iopub.status.busy": "2023-09-23T04:35:49.046646Z",
     "iopub.status.idle": "2023-09-23T04:35:49.058619Z",
     "shell.execute_reply": "2023-09-23T04:35:49.057596Z",
     "shell.execute_reply.started": "2023-09-23T04:35:49.04736Z"
    }
   },
   "outputs": [],
   "source": [
    "print(train.loc[train['unique_text'] == 4061])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 314 tweets that that are repeated more than once. There is a pretty good chance that some more of these may have different target codes for the same text. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-23T04:35:49.35175Z",
     "iopub.status.busy": "2023-09-23T04:35:49.35103Z",
     "iopub.status.idle": "2023-09-23T04:35:49.361379Z",
     "shell.execute_reply": "2023-09-23T04:35:49.360258Z",
     "shell.execute_reply.started": "2023-09-23T04:35:49.351711Z"
    }
   },
   "outputs": [],
   "source": [
    "train['unique_text'].value_counts().ne(1).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One way to correct this potential problem is to use the target mode for a set of duplicate tweets and change any targets that don't match to this mode value. For instance, in the example above for number 4061, the mode would be 0 and the 5 values that are not 0 would be changed to 0. \n",
    "\n",
    "To start this process a new dataframe is created to capture the mode for each unique tweet. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-23T04:35:52.424071Z",
     "iopub.status.busy": "2023-09-23T04:35:52.423679Z",
     "iopub.status.idle": "2023-09-23T04:35:55.101941Z",
     "shell.execute_reply": "2023-09-23T04:35:55.1009Z",
     "shell.execute_reply.started": "2023-09-23T04:35:52.424039Z"
    }
   },
   "outputs": [],
   "source": [
    "train_unique_mode = train.groupby('unique_text').agg({'target': lambda x: x.value_counts().index[0]}).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-23T04:35:55.110394Z",
     "iopub.status.busy": "2023-09-23T04:35:55.107716Z",
     "iopub.status.idle": "2023-09-23T04:35:55.1264Z",
     "shell.execute_reply": "2023-09-23T04:35:55.1254Z",
     "shell.execute_reply.started": "2023-09-23T04:35:55.110356Z"
    }
   },
   "outputs": [],
   "source": [
    "train_unique_mode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These mode values are then added as a new column called 'new_target' in the train dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-23T04:36:29.935409Z",
     "iopub.status.busy": "2023-09-23T04:36:29.935025Z",
     "iopub.status.idle": "2023-09-23T04:36:29.944295Z",
     "shell.execute_reply": "2023-09-23T04:36:29.943392Z",
     "shell.execute_reply.started": "2023-09-23T04:36:29.935375Z"
    }
   },
   "outputs": [],
   "source": [
    "train['new_target'] = train['unique_text'].map(train_unique_mode.set_index('unique_text')['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-23T04:36:32.812029Z",
     "iopub.status.busy": "2023-09-23T04:36:32.811557Z",
     "iopub.status.idle": "2023-09-23T04:36:32.842272Z",
     "shell.execute_reply": "2023-09-23T04:36:32.841294Z",
     "shell.execute_reply.started": "2023-09-23T04:36:32.811971Z"
    }
   },
   "outputs": [],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like there are 89 rows where the new target is not equal to the original target, which means 89 rows were changed based on looking at the mode of unique tweets with more than one occurance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-23T04:36:36.752141Z",
     "iopub.status.busy": "2023-09-23T04:36:36.751779Z",
     "iopub.status.idle": "2023-09-23T04:36:36.765458Z",
     "shell.execute_reply": "2023-09-23T04:36:36.764416Z",
     "shell.execute_reply.started": "2023-09-23T04:36:36.752108Z"
    }
   },
   "outputs": [],
   "source": [
    "len(train.query('new_target != target'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Back Translation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "nlpaug had a specific back translation function. It's default mode is to translate the source text to German and then translate it back to English. It seemed to work best by going through a list instead of a dataframe column, so the train_chunk list was created out of the 'clean_text' column. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-23T05:22:46.799725Z",
     "iopub.status.busy": "2023-09-23T05:22:46.799324Z",
     "iopub.status.idle": "2023-09-23T05:22:46.807586Z",
     "shell.execute_reply": "2023-09-23T05:22:46.806514Z",
     "shell.execute_reply.started": "2023-09-23T05:22:46.799696Z"
    }
   },
   "outputs": [],
   "source": [
    "train_chunk = train['clean_text']\n",
    "len(train_chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To download the actual model. The default device is CPU, but used 'cuda' to take advantage of Kaggle environment GPUs. The default batch_size was 32, but ran into a CUDA memory issue, so reduced it to 16. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-23T04:37:33.703885Z",
     "iopub.status.busy": "2023-09-23T04:37:33.703528Z",
     "iopub.status.idle": "2023-09-23T04:39:55.337039Z",
     "shell.execute_reply": "2023-09-23T04:39:55.336027Z",
     "shell.execute_reply.started": "2023-09-23T04:37:33.703856Z"
    }
   },
   "outputs": [],
   "source": [
    "back_trans_aug = naw.BackTranslationAug(device='cuda', batch_size=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned above it seems to work best iterating through a list, the translate list was created to hold the final results. It was wrapped in tqdm progress bar since the process took about 2 hours. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-23T05:23:30.379498Z",
     "iopub.status.busy": "2023-09-23T05:23:30.379097Z",
     "iopub.status.idle": "2023-09-23T07:16:08.862586Z",
     "shell.execute_reply": "2023-09-23T07:16:08.861176Z",
     "shell.execute_reply.started": "2023-09-23T05:23:30.379467Z"
    }
   },
   "outputs": [],
   "source": [
    "translate = []\n",
    "\n",
    "for i in tqdm(train_chunk):\n",
    "    row_translate = back_trans_aug.augment(i)\n",
    "    translate.append(row_translate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-23T07:16:08.865875Z",
     "iopub.status.busy": "2023-09-23T07:16:08.865477Z",
     "iopub.status.idle": "2023-09-23T07:16:08.873501Z",
     "shell.execute_reply": "2023-09-23T07:16:08.872577Z",
     "shell.execute_reply.started": "2023-09-23T07:16:08.865832Z"
    }
   },
   "outputs": [],
   "source": [
    "len(translate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-23T08:17:41.787591Z",
     "iopub.status.busy": "2023-09-23T08:17:41.787168Z",
     "iopub.status.idle": "2023-09-23T08:17:41.797088Z",
     "shell.execute_reply": "2023-09-23T08:17:41.795946Z",
     "shell.execute_reply.started": "2023-09-23T08:17:41.787556Z"
    }
   },
   "outputs": [],
   "source": [
    "translate[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create a new 'translate' column in the train dataframe with the results of the translation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-23T08:19:47.671678Z",
     "iopub.status.busy": "2023-09-23T08:19:47.671265Z",
     "iopub.status.idle": "2023-09-23T08:19:47.688412Z",
     "shell.execute_reply": "2023-09-23T08:19:47.687382Z",
     "shell.execute_reply.started": "2023-09-23T08:19:47.671645Z"
    }
   },
   "outputs": [],
   "source": [
    "train['translate'] = translate\n",
    "train['translate'] = train['translate'].str[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-23T08:19:49.715867Z",
     "iopub.status.busy": "2023-09-23T08:19:49.715107Z",
     "iopub.status.idle": "2023-09-23T08:19:49.736918Z",
     "shell.execute_reply": "2023-09-23T08:19:49.735846Z",
     "shell.execute_reply.started": "2023-09-23T08:19:49.715805Z"
    }
   },
   "outputs": [],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the backtranslation took about 2 hours, a csv file was saved to use in future notebooks without re-running the full process. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-23T07:21:53.463702Z",
     "iopub.status.busy": "2023-09-23T07:21:53.463289Z",
     "iopub.status.idle": "2023-09-23T07:21:53.605851Z",
     "shell.execute_reply": "2023-09-23T07:21:53.604858Z",
     "shell.execute_reply.started": "2023-09-23T07:21:53.46367Z"
    }
   },
   "outputs": [],
   "source": [
    "train.to_csv(\"train_translate.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To do the same backtranslating process on the test dataframe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-23T07:22:12.82417Z",
     "iopub.status.busy": "2023-09-23T07:22:12.823199Z",
     "iopub.status.idle": "2023-09-23T07:22:12.831198Z",
     "shell.execute_reply": "2023-09-23T07:22:12.830088Z",
     "shell.execute_reply.started": "2023-09-23T07:22:12.824134Z"
    }
   },
   "outputs": [],
   "source": [
    "test_chunk = test['clean_text']\n",
    "len(test_chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-23T07:22:17.914939Z",
     "iopub.status.busy": "2023-09-23T07:22:17.914583Z",
     "iopub.status.idle": "2023-09-23T08:11:02.957862Z",
     "shell.execute_reply": "2023-09-23T08:11:02.956538Z",
     "shell.execute_reply.started": "2023-09-23T07:22:17.91491Z"
    }
   },
   "outputs": [],
   "source": [
    "test_translate = []\n",
    "\n",
    "for i in tqdm(test_chunk):\n",
    "    row_translate = back_trans_aug.augment(i)\n",
    "    test_translate.append(row_translate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-23T08:20:32.524188Z",
     "iopub.status.busy": "2023-09-23T08:20:32.52319Z",
     "iopub.status.idle": "2023-09-23T08:20:32.531412Z",
     "shell.execute_reply": "2023-09-23T08:20:32.530414Z",
     "shell.execute_reply.started": "2023-09-23T08:20:32.524152Z"
    }
   },
   "outputs": [],
   "source": [
    "len(test_translate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-23T08:21:48.8643Z",
     "iopub.status.busy": "2023-09-23T08:21:48.863918Z",
     "iopub.status.idle": "2023-09-23T08:21:48.875971Z",
     "shell.execute_reply": "2023-09-23T08:21:48.874632Z",
     "shell.execute_reply.started": "2023-09-23T08:21:48.86427Z"
    }
   },
   "outputs": [],
   "source": [
    "test['translate'] = test_translate\n",
    "test['translate'] = test['translate'].str[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-23T08:21:51.412423Z",
     "iopub.status.busy": "2023-09-23T08:21:51.411304Z",
     "iopub.status.idle": "2023-09-23T08:21:51.428776Z",
     "shell.execute_reply": "2023-09-23T08:21:51.427551Z",
     "shell.execute_reply.started": "2023-09-23T08:21:51.412377Z"
    }
   },
   "outputs": [],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-23T08:21:55.356047Z",
     "iopub.status.busy": "2023-09-23T08:21:55.355296Z",
     "iopub.status.idle": "2023-09-23T08:21:55.418358Z",
     "shell.execute_reply": "2023-09-23T08:21:55.417346Z",
     "shell.execute_reply.started": "2023-09-23T08:21:55.356005Z"
    }
   },
   "outputs": [],
   "source": [
    "test.to_csv(\"test_translate.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing to Use the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The parameters from the starter notebook are used here and an 80/20 validation split is performed below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-23T08:22:00.516769Z",
     "iopub.status.busy": "2023-09-23T08:22:00.515747Z",
     "iopub.status.idle": "2023-09-23T08:22:00.529502Z",
     "shell.execute_reply": "2023-09-23T08:22:00.528381Z",
     "shell.execute_reply.started": "2023-09-23T08:22:00.516726Z"
    }
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "NUM_TRAINING_EXAMPLES = train.shape[0]\n",
    "TRAIN_SPLIT = 0.8\n",
    "VAL_SPLIT = 0.2\n",
    "STEPS_PER_EPOCH = int(NUM_TRAINING_EXAMPLES)*TRAIN_SPLIT // BATCH_SIZE\n",
    "\n",
    "EPOCHS = 2\n",
    "AUTO = tf.data.experimental.AUTOTUNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-23T08:22:03.008314Z",
     "iopub.status.busy": "2023-09-23T08:22:03.007882Z",
     "iopub.status.idle": "2023-09-23T08:22:03.019735Z",
     "shell.execute_reply": "2023-09-23T08:22:03.018684Z",
     "shell.execute_reply.started": "2023-09-23T08:22:03.008265Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = train[\"translate\"]\n",
    "y = train[\"new_target\"]\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=VAL_SPLIT, random_state=42)\n",
    "\n",
    "X_test = test[\"translate\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To ensure the results are the same for multiple iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-23T08:22:06.507914Z",
     "iopub.status.busy": "2023-09-23T08:22:06.50719Z",
     "iopub.status.idle": "2023-09-23T08:22:06.521543Z",
     "shell.execute_reply": "2023-09-23T08:22:06.520512Z",
     "shell.execute_reply.started": "2023-09-23T08:22:06.507879Z"
    }
   },
   "outputs": [],
   "source": [
    "def reset_seeds():\n",
    "   np.random.seed(42) \n",
    "   python_random.seed(42)\n",
    "   tf.random.set_seed(42)\n",
    "\n",
    "reset_seeds() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running the Model\n",
    "\n",
    "Text inputs need to be transformed to numeric token ids and arranged in several Tensors before being input to BERT.\n",
    "\n",
    "The BertClassifier model can be configured with a preprocessor layer, in which case it will automatically apply preprocessing to raw inputs during fit(), predict(), and evaluate(). This is done by default when creating the model with from_preset().\n",
    "\n",
    "The DistilBERT model that is chosen learns a distilled (approximate) version of BERT, retaining 97% performance but using only half the number of parameters ([paper](https://arxiv.org/abs/1910.01108)). \n",
    "\n",
    "It has 40% less parameters than bert-base-uncased, runs 60% faster while preserving over 95% of BERTâ€™s performances as measured on the GLUE language understanding benchmark.\n",
    "\n",
    "Specifically, it doesn't have token-type embeddings, pooler and retains only half of the layers from Google's BERT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-23T08:22:09.783869Z",
     "iopub.status.busy": "2023-09-23T08:22:09.783137Z",
     "iopub.status.idle": "2023-09-23T08:22:11.304197Z",
     "shell.execute_reply": "2023-09-23T08:22:11.303163Z",
     "shell.execute_reply.started": "2023-09-23T08:22:09.783831Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load a DistilBERT model.\n",
    "preset= \"distil_bert_base_en_uncased\"\n",
    "\n",
    "# Use a shorter sequence length.\n",
    "preprocessor = keras_nlp.models.DistilBertPreprocessor.from_preset(preset,\n",
    "                                                                   sequence_length=160,\n",
    "                                                                   name=\"preprocessor_4_tweets\"\n",
    "                                                                  )\n",
    "\n",
    "# Pretrained classifier.\n",
    "classifier = keras_nlp.models.DistilBertClassifier.from_preset(preset,\n",
    "                                                               preprocessor = preprocessor, \n",
    "                                                               num_classes=2)\n",
    "\n",
    "classifier.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-23T08:22:15.452041Z",
     "iopub.status.busy": "2023-09-23T08:22:15.451663Z",
     "iopub.status.idle": "2023-09-23T08:27:36.662904Z",
     "shell.execute_reply": "2023-09-23T08:27:36.661659Z",
     "shell.execute_reply.started": "2023-09-23T08:22:15.452001Z"
    }
   },
   "outputs": [],
   "source": [
    "# Compile\n",
    "classifier.compile(\n",
    "    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True), #'binary_crossentropy',\n",
    "    optimizer=keras.optimizers.Adam(1e-5),\n",
    "    metrics= [\"accuracy\"]  \n",
    ")\n",
    "\n",
    "# Fit\n",
    "history = classifier.fit(x=X_train,\n",
    "                         y=y_train,\n",
    "                         batch_size=BATCH_SIZE,\n",
    "                         epochs=EPOCHS, \n",
    "                         validation_data=(X_val, y_val)\n",
    "                        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submission "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-23T08:27:36.667371Z",
     "iopub.status.busy": "2023-09-23T08:27:36.666909Z",
     "iopub.status.idle": "2023-09-23T08:27:36.702751Z",
     "shell.execute_reply": "2023-09-23T08:27:36.701555Z",
     "shell.execute_reply.started": "2023-09-23T08:27:36.667316Z"
    }
   },
   "outputs": [],
   "source": [
    "def reset_seeds():\n",
    "   np.random.seed(42) \n",
    "   python_random.seed(42)\n",
    "   tf.random.set_seed(42)\n",
    "\n",
    "reset_seeds() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-23T08:27:36.70639Z",
     "iopub.status.busy": "2023-09-23T08:27:36.705593Z",
     "iopub.status.idle": "2023-09-23T08:27:36.740578Z",
     "shell.execute_reply": "2023-09-23T08:27:36.739409Z",
     "shell.execute_reply.started": "2023-09-23T08:27:36.706289Z"
    }
   },
   "outputs": [],
   "source": [
    "sample_submission = pd.read_csv(\"/kaggle/input/nlp-getting-started/sample_submission.csv\")\n",
    "sample_submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-23T08:27:36.74378Z",
     "iopub.status.busy": "2023-09-23T08:27:36.743111Z",
     "iopub.status.idle": "2023-09-23T08:28:00.369625Z",
     "shell.execute_reply": "2023-09-23T08:28:00.368507Z",
     "shell.execute_reply.started": "2023-09-23T08:27:36.743728Z"
    }
   },
   "outputs": [],
   "source": [
    "sample_submission[\"target\"] = np.argmax(classifier.predict(X_test), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-23T08:28:00.37169Z",
     "iopub.status.busy": "2023-09-23T08:28:00.371241Z",
     "iopub.status.idle": "2023-09-23T08:28:00.388436Z",
     "shell.execute_reply": "2023-09-23T08:28:00.387236Z",
     "shell.execute_reply.started": "2023-09-23T08:28:00.371648Z"
    }
   },
   "outputs": [],
   "source": [
    "sample_submission.to_csv(\"submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
