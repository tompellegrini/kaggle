{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Data Preparation and Text Cleaning","metadata":{}},{"cell_type":"markdown","source":"There are numerous data augmentation techniques that could improve predictions in NLP datasets. These can include \n\nSynonym Replacement: Randomly choose n words from the sentence that are not stop words. Replace each of these words with one of its synonyms chosen at random.\n\nRandom Insertion: Same as Synonym Replacement except it inserts the synonyms at random positions of the sentence. \n\nRandom Swap: Randomly choose two words in the sentence and swap their positions. \n\nRandom Deletion: Randomly remove each word in the sentence with probability p. \n\n\n\n\nIn this notebook the nlpaug package will be used: https://nlpaug.readthedocs.io/en/latest/. \n\n","metadata":{}},{"cell_type":"code","source":"!pip install keras-core --upgrade\n!pip install -q keras-nlp --upgrade\n!pip install nlpaug\n!pip install sacremoses\n\nimport os\nos.environ['KERAS_BACKEND'] = 'tensorflow'","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-09-24T07:59:36.883094Z","iopub.execute_input":"2023-09-24T07:59:36.883654Z","iopub.status.idle":"2023-09-24T08:00:38.592996Z","shell.execute_reply.started":"2023-09-24T07:59:36.883617Z","shell.execute_reply":"2023-09-24T08:00:38.591694Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nimport tensorflow as tf\nimport keras_core as keras\nimport keras_nlp\nimport nlpaug.augmenter.word as naw\nfrom tqdm import tqdm\n\nimport random as python_random\nimport re\nimport string\nimport emoji\n\n\nprint(\"TensorFlow version:\", tf.__version__)\nprint(\"KerasNLP version:\", keras_nlp.__version__)","metadata":{"execution":{"iopub.status.busy":"2023-09-24T08:07:57.371562Z","iopub.execute_input":"2023-09-24T08:07:57.371932Z","iopub.status.idle":"2023-09-24T08:07:59.541634Z","shell.execute_reply.started":"2023-09-24T08:07:57.371899Z","shell.execute_reply":"2023-09-24T08:07:59.539670Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This code was necessary to download the resources from nltk within the Kaggle kernal. ","metadata":{}},{"cell_type":"code","source":"import nltk\nimport subprocess\n\n# Download and unzip wordnet\ntry:\n    nltk.data.find('wordnet.zip')\nexcept:\n    nltk.download('wordnet', download_dir='/kaggle/working/')\n    command = \"unzip /kaggle/working/corpora/wordnet.zip -d /kaggle/working/corpora\"\n    subprocess.run(command.split())\n    nltk.data.path.append('/kaggle/working/')\n\n# Now you can import the NLTK resources as usual\nfrom nltk.corpus import wordnet","metadata":{"execution":{"iopub.status.busy":"2023-09-24T08:09:24.317931Z","iopub.execute_input":"2023-09-24T08:09:24.318313Z","iopub.status.idle":"2023-09-24T08:09:24.827911Z","shell.execute_reply.started":"2023-09-24T08:09:24.318283Z","shell.execute_reply":"2023-09-24T08:09:24.826774Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = pd.read_csv(\"/kaggle/input/nlp-getting-started/train.csv\")\ntest = pd.read_csv(\"/kaggle/input/nlp-getting-started/test.csv\")\n\nprint('Training Set Shape = {}'.format(train.shape))\nprint('Training Set Memory Usage = {:.2f} MB'.format(train.memory_usage().sum() / 1024**2))\nprint('Test Set Shape = {}'.format(test.shape))\nprint('Test Set Memory Usage = {:.2f} MB'.format(test.memory_usage().sum() / 1024**2))","metadata":{"execution":{"iopub.status.busy":"2023-09-24T08:01:07.162710Z","iopub.execute_input":"2023-09-24T08:01:07.163026Z","iopub.status.idle":"2023-09-24T08:01:07.253202Z","shell.execute_reply.started":"2023-09-24T08:01:07.162999Z","shell.execute_reply":"2023-09-24T08:01:07.252210Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.head()","metadata":{"execution":{"iopub.status.busy":"2023-09-24T08:01:07.255970Z","iopub.execute_input":"2023-09-24T08:01:07.256318Z","iopub.status.idle":"2023-09-24T08:01:07.276387Z","shell.execute_reply.started":"2023-09-24T08:01:07.256285Z","shell.execute_reply":"2023-09-24T08:01:07.275440Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test.head()","metadata":{"execution":{"iopub.status.busy":"2023-09-24T08:01:07.279427Z","iopub.execute_input":"2023-09-24T08:01:07.280394Z","iopub.status.idle":"2023-09-24T08:01:07.291066Z","shell.execute_reply.started":"2023-09-24T08:01:07.280359Z","shell.execute_reply":"2023-09-24T08:01:07.290011Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"A lot of the tweets in the dataset need to be cleaned up. Doing so should improve the results. In researching a way to clean up this text, the following Stack Overflow post was extremely helpful: https://stackoverflow.com/questions/64719706/cleaning-twitter-data-pandas-python","metadata":{}},{"cell_type":"code","source":"train_clean_tweets = []\nfor tweet in train['text']:\n    tweet = re.sub(\"@[A-Za-z0-9]+\",\"\",tweet) #Remove @ sign\n    tweet = re.sub(r\"(?:\\@|http?\\://|https?\\://|www)\\S+\", \"\", tweet) #Remove http links\n    tweet = \" \".join(tweet.split())\n    emojis = emoji.distinct_emoji_list(tweet)\n    tweet = ''.join(c for c in tweet if c not in emojis) #Remove Emojis\n    tweet = tweet.replace(\"#\", \"\").replace(\"_\", \" \") #Remove hashtag sign but keep the text\n    #tweet = \" \".join(w for w in nltk.wordpunct_tokenize(tweet) \\\n         #if w.lower() in tweet or not w.isalpha())\n    train_clean_tweets.append(tweet)\n    \ntrain['clean_text'] = train_clean_tweets","metadata":{"execution":{"iopub.status.busy":"2023-09-24T08:01:07.292766Z","iopub.execute_input":"2023-09-24T08:01:07.293461Z","iopub.status.idle":"2023-09-24T08:01:08.602138Z","shell.execute_reply.started":"2023-09-24T08:01:07.293426Z","shell.execute_reply":"2023-09-24T08:01:08.600800Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train","metadata":{"execution":{"iopub.status.busy":"2023-09-24T08:01:08.603475Z","iopub.execute_input":"2023-09-24T08:01:08.603839Z","iopub.status.idle":"2023-09-24T08:01:08.625409Z","shell.execute_reply.started":"2023-09-24T08:01:08.603793Z","shell.execute_reply":"2023-09-24T08:01:08.623103Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_clean_tweets = []\nfor tweet in test['text']:\n    tweet = re.sub(\"@[A-Za-z0-9]+\",\"\",tweet) #Remove @ sign\n    tweet = re.sub(r\"(?:\\@|http?\\://|https?\\://|www)\\S+\", \"\", tweet) #Remove http links\n    tweet = \" \".join(tweet.split())\n    emojis = emoji.distinct_emoji_list(tweet)\n    tweet = ''.join(c for c in tweet if c not in emojis) #Remove Emojis\n    tweet = tweet.replace(\"#\", \"\").replace(\"_\", \" \") #Remove hashtag sign but keep the text\n    #tweet = \" \".join(w for w in nltk.wordpunct_tokenize(tweet) \\\n         #if w.lower() in tweet or not w.isalpha())\n    test_clean_tweets.append(tweet)\n    \ntest['clean_text'] = test_clean_tweets","metadata":{"execution":{"iopub.status.busy":"2023-09-24T08:01:08.627777Z","iopub.execute_input":"2023-09-24T08:01:08.629445Z","iopub.status.idle":"2023-09-24T08:01:09.172675Z","shell.execute_reply.started":"2023-09-24T08:01:08.629405Z","shell.execute_reply":"2023-09-24T08:01:09.171711Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Varying Target Values for the Same Tweets","metadata":{}},{"cell_type":"markdown","source":"Looking at the number of unique values in each column of the train dataset it shows that there are 7613 total columns, but only 6922 of the input columns are unique, which is a total of 791 rows. That is a lot. The question whether a unique input value with many occurances are all labeled with the same target values.","metadata":{}},{"cell_type":"code","source":"train.nunique()","metadata":{"execution":{"iopub.status.busy":"2023-09-24T08:01:09.174110Z","iopub.execute_input":"2023-09-24T08:01:09.174434Z","iopub.status.idle":"2023-09-24T08:01:09.201867Z","shell.execute_reply.started":"2023-09-24T08:01:09.174402Z","shell.execute_reply":"2023-09-24T08:01:09.200743Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"To explore this potential labeling issue, a new column called 'unique_input' is created to be able to look at some of the larger occurances of unique values.","metadata":{}},{"cell_type":"code","source":"train['unique_text'] = pd.factorize(train['clean_text'])[0] + 1","metadata":{"execution":{"iopub.status.busy":"2023-09-24T08:01:09.205936Z","iopub.execute_input":"2023-09-24T08:01:09.206239Z","iopub.status.idle":"2023-09-24T08:01:09.215065Z","shell.execute_reply.started":"2023-09-24T08:01:09.206197Z","shell.execute_reply":"2023-09-24T08:01:09.214018Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train","metadata":{"execution":{"iopub.status.busy":"2023-09-24T08:01:09.216877Z","iopub.execute_input":"2023-09-24T08:01:09.217256Z","iopub.status.idle":"2023-09-24T08:01:09.237171Z","shell.execute_reply.started":"2023-09-24T08:01:09.217222Z","shell.execute_reply":"2023-09-24T08:01:09.236003Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Looking at the top five unique occurances, only the 4th one, 4061, had variations in the target values. It doesn't appear to be a disaster, but 5 out of 17 occurances were coded as a disaster.","metadata":{}},{"cell_type":"code","source":"train['unique_text'].value_counts().nlargest(5)","metadata":{"execution":{"iopub.status.busy":"2023-09-24T08:01:09.238774Z","iopub.execute_input":"2023-09-24T08:01:09.239211Z","iopub.status.idle":"2023-09-24T08:01:09.255174Z","shell.execute_reply.started":"2023-09-24T08:01:09.239171Z","shell.execute_reply":"2023-09-24T08:01:09.254188Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(train.loc[train['unique_text'] == 4061])","metadata":{"execution":{"iopub.status.busy":"2023-09-24T08:01:09.256469Z","iopub.execute_input":"2023-09-24T08:01:09.256908Z","iopub.status.idle":"2023-09-24T08:01:09.271505Z","shell.execute_reply.started":"2023-09-24T08:01:09.256872Z","shell.execute_reply":"2023-09-24T08:01:09.270648Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There are 314 tweets that that are repeated more than once. There is a pretty good chance that some more of these may have different target codes for the same text. ","metadata":{}},{"cell_type":"code","source":"train['unique_text'].value_counts().ne(1).sum()","metadata":{"execution":{"iopub.status.busy":"2023-09-24T08:01:09.272916Z","iopub.execute_input":"2023-09-24T08:01:09.273486Z","iopub.status.idle":"2023-09-24T08:01:09.282529Z","shell.execute_reply.started":"2023-09-24T08:01:09.273451Z","shell.execute_reply":"2023-09-24T08:01:09.281593Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"One way to correct this potential problem is to use the target mode for a set of duplicate tweets and change any targets that don't match to this mode value. For instance, in the example above for number 4061, the mode would be 0 and the 5 values that are not 0 would be changed to 0. \n\nTo start this process a new dataframe is created to capture the mode for each unique tweet. ","metadata":{}},{"cell_type":"code","source":"train_unique_mode = train.groupby('unique_text').agg({'target': lambda x: x.value_counts().index[0]}).reset_index()","metadata":{"execution":{"iopub.status.busy":"2023-09-24T08:01:09.283841Z","iopub.execute_input":"2023-09-24T08:01:09.284486Z","iopub.status.idle":"2023-09-24T08:01:11.263417Z","shell.execute_reply.started":"2023-09-24T08:01:09.284450Z","shell.execute_reply":"2023-09-24T08:01:11.262396Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_unique_mode","metadata":{"execution":{"iopub.status.busy":"2023-09-24T08:01:11.264673Z","iopub.execute_input":"2023-09-24T08:01:11.265056Z","iopub.status.idle":"2023-09-24T08:01:11.279230Z","shell.execute_reply.started":"2023-09-24T08:01:11.265023Z","shell.execute_reply":"2023-09-24T08:01:11.277797Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"These mode values are then added as a new column called 'new_target' in the train dataset. ","metadata":{}},{"cell_type":"code","source":"train['new_target'] = train['unique_text'].map(train_unique_mode.set_index('unique_text')['target'])","metadata":{"execution":{"iopub.status.busy":"2023-09-24T08:01:11.281286Z","iopub.execute_input":"2023-09-24T08:01:11.281732Z","iopub.status.idle":"2023-09-24T08:01:11.291218Z","shell.execute_reply.started":"2023-09-24T08:01:11.281696Z","shell.execute_reply":"2023-09-24T08:01:11.289986Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train","metadata":{"execution":{"iopub.status.busy":"2023-09-24T08:01:11.293067Z","iopub.execute_input":"2023-09-24T08:01:11.293511Z","iopub.status.idle":"2023-09-24T08:01:11.315610Z","shell.execute_reply.started":"2023-09-24T08:01:11.293400Z","shell.execute_reply":"2023-09-24T08:01:11.314484Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"It looks like there are 89 rows where the new target is not equal to the original target, which means 89 rows were changed based on looking at the mode of unique tweets with more than one occurance. ","metadata":{}},{"cell_type":"code","source":"len(train.query('new_target != target'))","metadata":{"execution":{"iopub.status.busy":"2023-09-24T08:01:11.317455Z","iopub.execute_input":"2023-09-24T08:01:11.317825Z","iopub.status.idle":"2023-09-24T08:01:11.330379Z","shell.execute_reply.started":"2023-09-24T08:01:11.317781Z","shell.execute_reply":"2023-09-24T08:01:11.329020Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Synonym Replacement","metadata":{}},{"cell_type":"markdown","source":"nlpaug has a synonym replacement function. It seemed to work best by going through a list instead of a dataframe column, so the train_chunk list was created out of the 'clean_text' column. ","metadata":{}},{"cell_type":"code","source":"train_chunk = train['clean_text']\nlen(train_chunk)","metadata":{"execution":{"iopub.status.busy":"2023-09-24T08:01:20.436021Z","iopub.execute_input":"2023-09-24T08:01:20.436497Z","iopub.status.idle":"2023-09-24T08:01:20.444433Z","shell.execute_reply.started":"2023-09-24T08:01:20.436455Z","shell.execute_reply":"2023-09-24T08:01:20.443265Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"To download the actual model. The default device is CPU, but used 'cuda' to take advantage of Kaggle environment GPUs. The default batch_size was 32, but ran into a CUDA memory issue, so reduced it to 16. ","metadata":{}},{"cell_type":"code","source":"synonym_replace_aug = naw.SynonymAug()","metadata":{"execution":{"iopub.status.busy":"2023-09-24T08:01:24.355303Z","iopub.execute_input":"2023-09-24T08:01:24.355664Z","iopub.status.idle":"2023-09-24T08:01:24.583615Z","shell.execute_reply.started":"2023-09-24T08:01:24.355634Z","shell.execute_reply":"2023-09-24T08:01:24.582641Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As mentioned above it seems to work best iterating through a list, the translate list was created to hold the final results. It was wrapped in tqdm progress bar since the process took about 2 hours. ","metadata":{}},{"cell_type":"code","source":"synonym = []\n\nfor i in tqdm(train_chunk):\n    row_synonym = synonym_replace_aug.augment(i)\n    synonym.append(row_synonym)","metadata":{"execution":{"iopub.status.busy":"2023-09-24T08:09:37.660293Z","iopub.execute_input":"2023-09-24T08:09:37.660652Z","iopub.status.idle":"2023-09-24T08:09:54.291928Z","shell.execute_reply.started":"2023-09-24T08:09:37.660624Z","shell.execute_reply":"2023-09-24T08:09:54.290958Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(synonym)","metadata":{"execution":{"iopub.status.busy":"2023-09-24T08:11:00.663651Z","iopub.execute_input":"2023-09-24T08:11:00.664086Z","iopub.status.idle":"2023-09-24T08:11:00.671288Z","shell.execute_reply.started":"2023-09-24T08:11:00.664051Z","shell.execute_reply":"2023-09-24T08:11:00.670187Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"synonym[0:10]","metadata":{"execution":{"iopub.status.busy":"2023-09-24T08:11:03.868156Z","iopub.execute_input":"2023-09-24T08:11:03.868630Z","iopub.status.idle":"2023-09-24T08:11:03.877481Z","shell.execute_reply.started":"2023-09-24T08:11:03.868584Z","shell.execute_reply":"2023-09-24T08:11:03.876448Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"To create a new 'translate' column in the train dataframe with the results of the translation. ","metadata":{}},{"cell_type":"code","source":"train['augment'] = synonym\ntrain['augment'] = train['augment'].str[0]","metadata":{"execution":{"iopub.status.busy":"2023-09-24T08:11:07.611537Z","iopub.execute_input":"2023-09-24T08:11:07.612012Z","iopub.status.idle":"2023-09-24T08:11:07.636016Z","shell.execute_reply.started":"2023-09-24T08:11:07.611946Z","shell.execute_reply":"2023-09-24T08:11:07.634998Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train","metadata":{"execution":{"iopub.status.busy":"2023-09-24T08:11:10.448029Z","iopub.execute_input":"2023-09-24T08:11:10.448462Z","iopub.status.idle":"2023-09-24T08:11:10.476114Z","shell.execute_reply.started":"2023-09-24T08:11:10.448427Z","shell.execute_reply":"2023-09-24T08:11:10.474911Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Since the backtranslation took about 2 hours, a csv file was saved to use in future notebooks without re-running the full process. ","metadata":{}},{"cell_type":"code","source":"#train.to_csv(\"train_augment.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2023-09-23T07:21:53.463289Z","iopub.execute_input":"2023-09-23T07:21:53.463702Z","iopub.status.idle":"2023-09-23T07:21:53.605851Z","shell.execute_reply.started":"2023-09-23T07:21:53.46367Z","shell.execute_reply":"2023-09-23T07:21:53.604858Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"To do the same backtranslating process on the test dataframe. ","metadata":{}},{"cell_type":"code","source":"test_chunk = test['clean_text']\nlen(test_chunk)","metadata":{"execution":{"iopub.status.busy":"2023-09-24T08:11:29.347510Z","iopub.execute_input":"2023-09-24T08:11:29.347872Z","iopub.status.idle":"2023-09-24T08:11:29.354590Z","shell.execute_reply.started":"2023-09-24T08:11:29.347841Z","shell.execute_reply":"2023-09-24T08:11:29.353445Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_synonym = []\n\nfor i in tqdm(test_chunk):\n    row_synonym = synonym_replace_aug.augment(i)\n    test_synonym.append(row_synonym)","metadata":{"execution":{"iopub.status.busy":"2023-09-24T08:11:33.129590Z","iopub.execute_input":"2023-09-24T08:11:33.130048Z","iopub.status.idle":"2023-09-24T08:11:39.001900Z","shell.execute_reply.started":"2023-09-24T08:11:33.130012Z","shell.execute_reply":"2023-09-24T08:11:39.000970Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(test_synonym)","metadata":{"execution":{"iopub.status.busy":"2023-09-24T08:11:44.439721Z","iopub.execute_input":"2023-09-24T08:11:44.440130Z","iopub.status.idle":"2023-09-24T08:11:44.446725Z","shell.execute_reply.started":"2023-09-24T08:11:44.440097Z","shell.execute_reply":"2023-09-24T08:11:44.445596Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test['augment'] = test_synonym\ntest['augment'] = test['augment'].str[0]","metadata":{"execution":{"iopub.status.busy":"2023-09-24T08:11:47.987631Z","iopub.execute_input":"2023-09-24T08:11:47.988002Z","iopub.status.idle":"2023-09-24T08:11:47.998178Z","shell.execute_reply.started":"2023-09-24T08:11:47.987967Z","shell.execute_reply":"2023-09-24T08:11:47.996919Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test","metadata":{"execution":{"iopub.status.busy":"2023-09-24T08:11:50.656403Z","iopub.execute_input":"2023-09-24T08:11:50.656769Z","iopub.status.idle":"2023-09-24T08:11:50.682677Z","shell.execute_reply.started":"2023-09-24T08:11:50.656740Z","shell.execute_reply":"2023-09-24T08:11:50.681553Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#test.to_csv(\"test_augment.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2023-09-23T08:21:55.355296Z","iopub.execute_input":"2023-09-23T08:21:55.356047Z","iopub.status.idle":"2023-09-23T08:21:55.418358Z","shell.execute_reply.started":"2023-09-23T08:21:55.356005Z","shell.execute_reply":"2023-09-23T08:21:55.417346Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Preparing to Use the Model","metadata":{}},{"cell_type":"markdown","source":"The parameters from the starter notebook are used here and an 80/20 validation split is performed below. ","metadata":{}},{"cell_type":"code","source":"BATCH_SIZE = 32\nNUM_TRAINING_EXAMPLES = train.shape[0]\nTRAIN_SPLIT = 0.8\nVAL_SPLIT = 0.2\nSTEPS_PER_EPOCH = int(NUM_TRAINING_EXAMPLES)*TRAIN_SPLIT // BATCH_SIZE\n\nEPOCHS = 2\nAUTO = tf.data.experimental.AUTOTUNE","metadata":{"execution":{"iopub.status.busy":"2023-09-24T08:12:02.932130Z","iopub.execute_input":"2023-09-24T08:12:02.932522Z","iopub.status.idle":"2023-09-24T08:12:02.938586Z","shell.execute_reply.started":"2023-09-24T08:12:02.932488Z","shell.execute_reply":"2023-09-24T08:12:02.937306Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nX = train[\"augment\"]\ny = train[\"new_target\"]\n\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size=VAL_SPLIT, random_state=42)\n\nX_test = test[\"augment\"]","metadata":{"execution":{"iopub.status.busy":"2023-09-24T08:12:06.388805Z","iopub.execute_input":"2023-09-24T08:12:06.389728Z","iopub.status.idle":"2023-09-24T08:12:06.400306Z","shell.execute_reply.started":"2023-09-24T08:12:06.389681Z","shell.execute_reply":"2023-09-24T08:12:06.399168Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"To ensure the results are the same for multiple iterations","metadata":{}},{"cell_type":"code","source":"def reset_seeds():\n   np.random.seed(42) \n   python_random.seed(42)\n   tf.random.set_seed(42)\n\nreset_seeds() ","metadata":{"execution":{"iopub.status.busy":"2023-09-24T08:12:12.719581Z","iopub.execute_input":"2023-09-24T08:12:12.720034Z","iopub.status.idle":"2023-09-24T08:12:12.726090Z","shell.execute_reply.started":"2023-09-24T08:12:12.720000Z","shell.execute_reply":"2023-09-24T08:12:12.725081Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Running the Model\n\nText inputs need to be transformed to numeric token ids and arranged in several Tensors before being input to BERT.\n\nThe BertClassifier model can be configured with a preprocessor layer, in which case it will automatically apply preprocessing to raw inputs during fit(), predict(), and evaluate(). This is done by default when creating the model with from_preset().\n\nThe DistilBERT model that is chosen learns a distilled (approximate) version of BERT, retaining 97% performance but using only half the number of parameters ([paper](https://arxiv.org/abs/1910.01108)). \n\nIt has 40% less parameters than bert-base-uncased, runs 60% faster while preserving over 95% of BERT’s performances as measured on the GLUE language understanding benchmark.\n\nSpecifically, it doesn't have token-type embeddings, pooler and retains only half of the layers from Google's BERT.","metadata":{}},{"cell_type":"code","source":"# Load a DistilBERT model.\npreset= \"distil_bert_base_en_uncased\"\n\n# Use a shorter sequence length.\npreprocessor = keras_nlp.models.DistilBertPreprocessor.from_preset(preset,\n                                                                   sequence_length=160,\n                                                                   name=\"preprocessor_4_tweets\"\n                                                                  )\n\n# Pretrained classifier.\nclassifier = keras_nlp.models.DistilBertClassifier.from_preset(preset,\n                                                               preprocessor = preprocessor, \n                                                               num_classes=2)\n\nclassifier.summary()","metadata":{"execution":{"iopub.status.busy":"2023-09-24T08:12:16.487850Z","iopub.execute_input":"2023-09-24T08:12:16.488523Z","iopub.status.idle":"2023-09-24T08:12:31.810854Z","shell.execute_reply.started":"2023-09-24T08:12:16.488488Z","shell.execute_reply":"2023-09-24T08:12:31.809858Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Compile\nclassifier.compile(\n    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True), #'binary_crossentropy',\n    optimizer=keras.optimizers.Adam(1e-5),\n    metrics= [\"accuracy\"]  \n)\n\n# Fit\nhistory = classifier.fit(x=X_train,\n                         y=y_train,\n                         batch_size=BATCH_SIZE,\n                         epochs=EPOCHS, \n                         validation_data=(X_val, y_val)\n                        )","metadata":{"execution":{"iopub.status.busy":"2023-09-24T08:12:36.943552Z","iopub.execute_input":"2023-09-24T08:12:36.943933Z","iopub.status.idle":"2023-09-24T08:18:01.860411Z","shell.execute_reply.started":"2023-09-24T08:12:36.943902Z","shell.execute_reply":"2023-09-24T08:18:01.859331Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Submission ","metadata":{}},{"cell_type":"code","source":"def reset_seeds():\n   np.random.seed(42) \n   python_random.seed(42)\n   tf.random.set_seed(42)\n\nreset_seeds() ","metadata":{"execution":{"iopub.status.busy":"2023-09-24T08:18:37.349323Z","iopub.execute_input":"2023-09-24T08:18:37.349791Z","iopub.status.idle":"2023-09-24T08:18:37.394462Z","shell.execute_reply.started":"2023-09-24T08:18:37.349752Z","shell.execute_reply":"2023-09-24T08:18:37.393251Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample_submission = pd.read_csv(\"/kaggle/input/nlp-getting-started/sample_submission.csv\")\nsample_submission.head()","metadata":{"execution":{"iopub.status.busy":"2023-09-24T08:18:40.528800Z","iopub.execute_input":"2023-09-24T08:18:40.529305Z","iopub.status.idle":"2023-09-24T08:18:40.555436Z","shell.execute_reply.started":"2023-09-24T08:18:40.529267Z","shell.execute_reply":"2023-09-24T08:18:40.554458Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample_submission[\"target\"] = np.argmax(classifier.predict(X_test), axis=1)","metadata":{"execution":{"iopub.status.busy":"2023-09-24T08:18:44.172408Z","iopub.execute_input":"2023-09-24T08:18:44.172877Z","iopub.status.idle":"2023-09-24T08:19:27.254100Z","shell.execute_reply.started":"2023-09-24T08:18:44.172837Z","shell.execute_reply":"2023-09-24T08:19:27.252833Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample_submission.to_csv(\"submission.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2023-09-24T08:19:27.256904Z","iopub.execute_input":"2023-09-24T08:19:27.257365Z","iopub.status.idle":"2023-09-24T08:19:27.279839Z","shell.execute_reply.started":"2023-09-24T08:19:27.257322Z","shell.execute_reply":"2023-09-24T08:19:27.278783Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}