{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a13b007f",
   "metadata": {},
   "source": [
    "# Data Preprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "789f7c1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "from numpy import random\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_squared_log_error\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "\n",
    "from scipy.stats import skew, norm\n",
    "from feature_engine.transformation import YeoJohnsonTransformer\n",
    "from category_encoders.binary import BinaryEncoder\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ef65eb9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.svm import SVR\n",
    "from lightgbm import LGBMRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from mlxtend.regressor import StackingRegressor\n",
    "from sklearn.linear_model import LinearRegression, BayesianRidge\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from mlxtend.regressor import StackingCVRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0e8826f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import mutual_info_regression\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3af42dbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv('test.csv')\n",
    "sample = pd.read_csv('sample_submission.csv')\n",
    "train = pd.read_csv('train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1e7680c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1460, 81), (1459, 80))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape, test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50c5e835",
   "metadata": {},
   "source": [
    "Based on an initial analysis the train and test datasets have similar characteristics, so it will be easier to combine them for the data preprocessing work. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e1e76d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test = pd.concat([train, test], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d3aec0de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2919, 81)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ade7251",
   "metadata": {},
   "source": [
    "To show the amount of 'NaN' values in each column of the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "331f1180",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MSZoning           4\n",
       "LotFrontage      486\n",
       "Alley           2721\n",
       "Utilities          2\n",
       "Exterior1st        1\n",
       "Exterior2nd        1\n",
       "MasVnrType        24\n",
       "MasVnrArea        23\n",
       "BsmtQual          81\n",
       "BsmtCond          82\n",
       "BsmtExposure      82\n",
       "BsmtFinType1      79\n",
       "BsmtFinSF1         1\n",
       "BsmtFinType2      80\n",
       "BsmtFinSF2         1\n",
       "BsmtUnfSF          1\n",
       "TotalBsmtSF        1\n",
       "Electrical         1\n",
       "BsmtFullBath       2\n",
       "BsmtHalfBath       2\n",
       "KitchenQual        1\n",
       "Functional         2\n",
       "FireplaceQu     1420\n",
       "GarageType       157\n",
       "GarageYrBlt      159\n",
       "GarageFinish     159\n",
       "GarageCars         1\n",
       "GarageArea         1\n",
       "GarageQual       159\n",
       "GarageCond       159\n",
       "PoolQC          2909\n",
       "Fence           2348\n",
       "MiscFeature     2814\n",
       "SaleType           1\n",
       "SalePrice       1459\n",
       "dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.isnull(train_test).sum()[pd.isnull(train_test).sum() > 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0d0316a",
   "metadata": {},
   "source": [
    "It looks like Alley, FireplaceQu, PoolQC, Fence and MiscFeature have significant numbers of missing data. At first those columns were eliminated. However, the results were better if these columns were retained and the missing data were coded as 'None', assuming these features didn't exist for their related houses. \n",
    "\n",
    "There are a number of rows that have less than 5 rows with missing data. Since some of these are categorical and some are continuous data, their missing data will be replaced with the most frequent value. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "29ab3f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_high_nan=['Alley','FireplaceQu','PoolQC','Fence','MiscFeature']\n",
    "train_test[drop_high_nan] = train_test[drop_high_nan].fillna('None')\n",
    "\n",
    "small_nan_cols = ['MSZoning', 'Utilities', 'Exterior1st', 'Exterior2nd', 'BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF', \n",
    "                  'TotalBsmtSF', 'Electrical', 'BsmtFullBath', 'BsmtHalfBath', 'KitchenQual', 'Functional', 'GarageCars', \n",
    "                  'GarageArea','SaleType', 'SaleCondition']\n",
    "small_impute = SimpleImputer(strategy='most_frequent')\n",
    "train_test[small_nan_cols] = pd.DataFrame(small_impute.fit_transform(train_test[small_nan_cols]),columns=small_nan_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f1be893",
   "metadata": {},
   "source": [
    "The following columns seem to have one value significantly larger than the rest, and it would probably be best to use the mode, or most common, value to feel each NaN value: MasVnrType, MasVnrArea, BsmtCond, BsmtExposure, BsmtFinType2, GarageType, GarageFinish, GarageQual, and GarageCond. That represents 9 out of the 13 columns. \n",
    "\n",
    "BsmtQual has two values larger than the rest: Gd and TA. But it only has 2.8% NaNs, so simply using the mode might be good enough.  \n",
    "\n",
    "GarageYrBlt has 59 NaNs out 2919 rows which is only 2%. It has a dispersed set of values, so it might be easiest just to have any NaNs have the same value as YearBuilt. \n",
    "\n",
    "BsmtFinType1 has only 2.7% value of NaNs, and most two of its largest values are GLQ and Unf. It might be easiest to use the mode here. \n",
    "\n",
    "LotFrontage has 486 NaNs out of 2919 rows which is a pretty high 16.7%. It has a dispersed range of values, but looking at its characteristics from the describe function above, it seems to have a pretty even distribution with a mean of 10,168 and a median of 9,453. So using the mean to fill in the NaNs seems reasonable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c4271ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "mode_cols = ['MasVnrType', 'MasVnrArea', 'BsmtCond', 'BsmtExposure', 'BsmtFinType2', 'BsmtQual', 'BsmtFinType1']\n",
    "mode_impute = SimpleImputer(strategy='most_frequent')\n",
    "train_test[mode_cols] = pd.DataFrame(mode_impute.fit_transform(train_test[mode_cols]),columns=mode_cols)\n",
    "\n",
    "garage_cols = ['GarageType', 'GarageFinish', 'GarageQual','GarageCond', 'GarageYrBlt']\n",
    "train_test[garage_cols] = train_test[garage_cols].fillna('None')\n",
    "\n",
    "train_test['LotFrontage'].fillna((train_test['LotFrontage'].mean()), inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bfd9313",
   "metadata": {},
   "source": [
    "A final check to ensure that the only missing values are the expected SalePrice data from the test dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "80091d42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SalePrice    1459\n",
       "dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.isnull(train_test).sum()[pd.isnull(train_test).sum() > 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6dd5817",
   "metadata": {},
   "source": [
    "The BsmtQual and BsmtFinType1 have ordinal scaled categorical values with an inherent order. So the conversion from categorical to numerical was done manually. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "46b9042f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Gd', 'TA', 'Ex', 'Fa'], dtype=object)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_test['BsmtQual'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "68371091",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['GLQ', 'ALQ', 'Unf', 'Rec', 'BLQ', 'LwQ'], dtype=object)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_test['BsmtFinType1'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "12457a45",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test.BsmtQual = train_test.BsmtQual.replace({\"Ex\": 110, \"Gd\": 95, \"TA\": 85, \"Fa\": 75, \"Po\": 60, \"NA\": 0})\n",
    "train_test.BsmtFinType1 = train_test.BsmtFinType1.replace({\"GLQ\": 6, \"ALQ\": 5, \"BLQ\": 4, \"Rec\": 3, \"LwQ\": 2, \"Unf\": 1,\n",
    "                                                         \"NA\": 0})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a231025",
   "metadata": {},
   "source": [
    "To look at how many features have a skew above 0.6, since high skew can be an issue in regression analysis. For some reason, the results were better with a skew limit of 0.6 than the more standard value of 0.5. Skewness measures how symmetrical a distribution of data is. Data with a skew of 0 is perfectly symmetrical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cc1a4abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "number_cols = train_test.select_dtypes(include=np.number).columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "96c88d9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['MiscVal', 'PoolArea', 'LotArea', 'LowQualFinSF', '3SsnPorch',\n",
       "       'KitchenAbvGr', 'EnclosedPorch', 'ScreenPorch', 'OpenPorchSF',\n",
       "       'WoodDeckSF', 'LotFrontage', '1stFlrSF', 'MSSubClass', 'GrLivArea',\n",
       "       '2ndFlrSF', 'BsmtQual', 'TotRmsAbvGrd', 'Fireplaces', 'HalfBath'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "skew_features = train_test[number_cols].apply(lambda x: skew(x)).sort_values(ascending=False)\n",
    "high_skew = skew_features[skew_features > 0.6]\n",
    "skew_index = high_skew.index\n",
    "skew_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d624c669",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 19 numerical features with Skew > 0.6 :\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Skew</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>MiscVal</th>\n",
       "      <td>21.947195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PoolArea</th>\n",
       "      <td>16.898328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LotArea</th>\n",
       "      <td>12.822431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LowQualFinSF</th>\n",
       "      <td>12.088761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3SsnPorch</th>\n",
       "      <td>11.376065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>KitchenAbvGr</th>\n",
       "      <td>4.302254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EnclosedPorch</th>\n",
       "      <td>4.003891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ScreenPorch</th>\n",
       "      <td>3.946694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>OpenPorchSF</th>\n",
       "      <td>2.535114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>WoodDeckSF</th>\n",
       "      <td>1.842433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LotFrontage</th>\n",
       "      <td>1.645574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1stFlrSF</th>\n",
       "      <td>1.469604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MSSubClass</th>\n",
       "      <td>1.375457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GrLivArea</th>\n",
       "      <td>1.269358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2ndFlrSF</th>\n",
       "      <td>0.861675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BsmtQual</th>\n",
       "      <td>0.796302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TotRmsAbvGrd</th>\n",
       "      <td>0.758367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Fireplaces</th>\n",
       "      <td>0.733495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HalfBath</th>\n",
       "      <td>0.694566</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Skew\n",
       "MiscVal        21.947195\n",
       "PoolArea       16.898328\n",
       "LotArea        12.822431\n",
       "LowQualFinSF   12.088761\n",
       "3SsnPorch      11.376065\n",
       "KitchenAbvGr    4.302254\n",
       "EnclosedPorch   4.003891\n",
       "ScreenPorch     3.946694\n",
       "OpenPorchSF     2.535114\n",
       "WoodDeckSF      1.842433\n",
       "LotFrontage     1.645574\n",
       "1stFlrSF        1.469604\n",
       "MSSubClass      1.375457\n",
       "GrLivArea       1.269358\n",
       "2ndFlrSF        0.861675\n",
       "BsmtQual        0.796302\n",
       "TotRmsAbvGrd    0.758367\n",
       "Fireplaces      0.733495\n",
       "HalfBath        0.694566"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"There are {} numerical features with Skew > 0.6 :\".format(high_skew.shape[0]))\n",
    "skewness = pd.DataFrame({'Skew' :high_skew})\n",
    "skewness"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd9ecf2b",
   "metadata": {},
   "source": [
    "To normalize the features with skew above 0.6 the Yeo-Johnson Transformation is used since data with negative values or values of zero can be included. This transformation is a way to transform a continuous variable so that the output looks more normally distributed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ca19eae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "yjt = YeoJohnsonTransformer()\n",
    "yjt.fit(train_test[skew_index])\n",
    "train_test[skew_index] = yjt.transform(train_test[skew_index])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2cd0509",
   "metadata": {},
   "source": [
    "Since most models cannot work with categorical data, it will be necessary to identify all the columns that have non-numeric object values and then convert them to numeric values. The best approach for this conversion seems to be the straightforward python factorize function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "80aaa6ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "obj_cols = list(train_test.select_dtypes(['object']).columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "62baae01",
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in obj_cols:\n",
    "     train_test[column] = pd.factorize(train_test[column], sort=True)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "086c88f7",
   "metadata": {},
   "source": [
    "To create some new columns that might compound the effects of some of the existing columns which have higher impacts the score. eli5 (https://eli5.readthedocs.io/en/latest/overview.html) was used with an xgbregressor model with its default parameters on an earlier workbook version to measure the impact of each column on the overall score. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "36eb99a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test['QualCondSum'] = train_test['OverallQual'] + train_test['OverallCond']\n",
    "train_test['RemodTime'] = train_test['YearRemodAdd'] - train_test['YearBuilt']\n",
    "train_test['BsmtFinTypeSF1'] = train_test['BsmtFinType1'] * train_test['BsmtFinSF1']\n",
    "train_test['TotalFlrSF'] = train_test['1stFlrSF'] + train_test['2ndFlrSF']\n",
    "train_test['TotalFinSF'] = train_test['GrLivArea'] + train_test['BsmtFinSF1']\n",
    "train_test['GarageCarArea'] = train_test['GarageArea'] * train_test['GarageCars']\n",
    "train_test['TotalSF'] = train_test['1stFlrSF'] + train_test['2ndFlrSF'] + train_test['TotalBsmtSF']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1680ebf0",
   "metadata": {},
   "source": [
    "# Additional Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ab1c4d8",
   "metadata": {},
   "source": [
    "After completing this model, I took the Kaggle Feature Engineering Class: https://www.kaggle.com/learn/feature-engineering\n",
    "\n",
    "Taking ideas from that class the following additional features were created. \n",
    "\n",
    "The 'Features' feature was created by going through the feature list and finding features that could have a value of 0 and combining the scores of each feature that didn't have a 0. \n",
    "\n",
    "The 'MedNhbdArea' feature was derived from two of the most impactful features from doing a Mutual Information score analysis and grouping them to show the average GrLivArea for each Neighborhood.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f137a706",
   "metadata": {},
   "outputs": [],
   "source": [
    "Features = ['TotalBsmtSF', 'CentralAir', '2ndFlrSF', 'Fireplaces', 'GarageArea', 'WoodDeckSF', 'PoolArea', 'MiscVal']\n",
    "train_test['Features'] = train_test[Features].gt(0).sum(axis=1)\n",
    "\n",
    "train_test['MedNhbdArea'] = (train_test.groupby('Neighborhood')['GrLivArea'].transform('median')) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "391638b9",
   "metadata": {},
   "source": [
    "The 'Cluster' feature was created using K-Means. The space columns were some of the highest scoring Mutual Information features. They are then scaled, and a K-means analysis was run on them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "86863533",
   "metadata": {},
   "outputs": [],
   "source": [
    "space = ['LotArea', 'TotalBsmtSF', '1stFlrSF', '2ndFlrSF', 'GrLivArea', 'GarageArea']\n",
    "\n",
    "train_test_scaled = train_test.loc[:, space]\n",
    "train_test_scaled = (train_test_scaled - train_test_scaled.mean(axis=0)) / train_test_scaled.std(axis=0)\n",
    "\n",
    "kmeans = KMeans(n_clusters=10, random_state=0)\n",
    "train_test['Cluster'] = kmeans.fit_predict(train_test_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dafb2c5",
   "metadata": {},
   "source": [
    "Next, each of the 10 K-means clusters were used to create 10 new Centroid features based the cluster distances. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "80932328",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Centroid_0</th>\n",
       "      <th>Centroid_1</th>\n",
       "      <th>Centroid_2</th>\n",
       "      <th>Centroid_3</th>\n",
       "      <th>Centroid_4</th>\n",
       "      <th>Centroid_5</th>\n",
       "      <th>Centroid_6</th>\n",
       "      <th>Centroid_7</th>\n",
       "      <th>Centroid_8</th>\n",
       "      <th>Centroid_9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.805477</td>\n",
       "      <td>4.056948</td>\n",
       "      <td>2.918934</td>\n",
       "      <td>1.371359</td>\n",
       "      <td>3.092016</td>\n",
       "      <td>3.354219</td>\n",
       "      <td>2.737796</td>\n",
       "      <td>4.613459</td>\n",
       "      <td>2.630429</td>\n",
       "      <td>0.727621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.203703</td>\n",
       "      <td>2.481990</td>\n",
       "      <td>1.163126</td>\n",
       "      <td>2.948924</td>\n",
       "      <td>0.781183</td>\n",
       "      <td>2.571052</td>\n",
       "      <td>3.407897</td>\n",
       "      <td>3.729459</td>\n",
       "      <td>1.440709</td>\n",
       "      <td>2.752441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.536584</td>\n",
       "      <td>3.683415</td>\n",
       "      <td>3.077031</td>\n",
       "      <td>1.904208</td>\n",
       "      <td>3.041460</td>\n",
       "      <td>3.746463</td>\n",
       "      <td>2.217210</td>\n",
       "      <td>4.016120</td>\n",
       "      <td>2.690726</td>\n",
       "      <td>0.576227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.320031</td>\n",
       "      <td>3.894243</td>\n",
       "      <td>3.161313</td>\n",
       "      <td>1.941166</td>\n",
       "      <td>3.178007</td>\n",
       "      <td>3.675633</td>\n",
       "      <td>2.500895</td>\n",
       "      <td>4.346580</td>\n",
       "      <td>2.627652</td>\n",
       "      <td>0.844579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.968461</td>\n",
       "      <td>3.005393</td>\n",
       "      <td>4.072529</td>\n",
       "      <td>3.502604</td>\n",
       "      <td>3.319298</td>\n",
       "      <td>5.060120</td>\n",
       "      <td>1.077796</td>\n",
       "      <td>3.402015</td>\n",
       "      <td>3.517645</td>\n",
       "      <td>1.913878</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Centroid_0  Centroid_1  Centroid_2  Centroid_3  Centroid_4  Centroid_5  \\\n",
       "0    2.805477    4.056948    2.918934    1.371359    3.092016    3.354219   \n",
       "1    4.203703    2.481990    1.163126    2.948924    0.781183    2.571052   \n",
       "2    3.536584    3.683415    3.077031    1.904208    3.041460    3.746463   \n",
       "3    3.320031    3.894243    3.161313    1.941166    3.178007    3.675633   \n",
       "4    4.968461    3.005393    4.072529    3.502604    3.319298    5.060120   \n",
       "\n",
       "   Centroid_6  Centroid_7  Centroid_8  Centroid_9  \n",
       "0    2.737796    4.613459    2.630429    0.727621  \n",
       "1    3.407897    3.729459    1.440709    2.752441  \n",
       "2    2.217210    4.016120    2.690726    0.576227  \n",
       "3    2.500895    4.346580    2.627652    0.844579  \n",
       "4    1.077796    3.402015    3.517645    1.913878  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_test_cd = kmeans.fit_transform(train_test_scaled)\n",
    "\n",
    "train_test_cd = pd.DataFrame(train_test_cd, columns=[f\"Centroid_{i}\" for i in range(train_test_cd.shape[1])])\n",
    "train_test = train_test.join(train_test_cd)\n",
    "train_test_cd.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "278e13db",
   "metadata": {},
   "source": [
    "Principal Component Analysis was used with eight of the top Mutual Information scoring features. A number of the components showed potential inverse relationships between two features, hence the new features below were created. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "728d1623",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test['Neighbor_or_YearBlt'] = train_test['Neighborhood'] * train_test['YearBuilt']\n",
    "train_test['GrLivArea_or_1stFlrSF'] = train_test['GrLivArea'] * train_test['1stFlrSF']\n",
    "train_test['YearBlt_or_BsmtQual'] = train_test['YearBuilt'] * train_test['BsmtQual']\n",
    "train_test['BrLivArea_or_GarageArea'] = train_test['GrLivArea'] * train_test['GarageArea']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec85e4ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "48390e60",
   "metadata": {},
   "source": [
    "To create a column with the log of the SalePrice to match the evaluation metric in the contest. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "77ad85f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test['LogSalePrice'] = train_test['SalePrice'].apply(np.log)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "423bcc24",
   "metadata": {},
   "source": [
    "#  Setting Up and Running the Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1baca8c9",
   "metadata": {},
   "source": [
    "To separate the train_test dataset back into the train and test datasets and identify the independent and dependent columns. Because the dataset is so small, the cross fold validation process seemed to have much less overfitting than creating the separate validation and training sets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c5bba200",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train_test[train_test['SalePrice'].notnull()].copy()\n",
    "test = train_test[train_test['SalePrice'].isnull()].drop(['SalePrice','LogSalePrice'],axis=1)\n",
    "X = train.drop(['SalePrice','LogSalePrice'],axis=1)\n",
    "y = train.LogSalePrice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "039b618c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X,y = shuffle(X,y, random_state=42)\n",
    "X = X.reset_index(drop=True)\n",
    "y = y.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34d28dd1",
   "metadata": {},
   "source": [
    "To set up the cross validation folds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "15938fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = KFold(n_splits=12, random_state=42, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9ef82ec",
   "metadata": {},
   "source": [
    "In a separate workbook, Optuna (https://optuna.org/) was used to find the optimal parameters for a wide selection of regression models. It was interesting that the results using Optuna on this dataset were not significantly better than using the default parameters for most of the models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8f3e93ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_model = CatBoostRegressor(colsample_bylevel= 0.08309602563537534, learning_rate= 0.08286145675756133, depth= 4, \n",
    "                              l2_leaf_reg= 14.555249413444315, subsample= 0.9097411584295835, \n",
    "                              bagging_temperature= 3.177590955252409, model_size_reg= 0.3808343022980778, \n",
    "                              boosting_type= 'Plain', verbose=False, random_state=42)\n",
    "\n",
    "xgb_model = xgb.XGBRegressor(reg_lambda= 5.096430599223015, alpha= 1.3223698329753615, \n",
    "                             colsample_bytree= 0.10366646849522436, subsample= 0.9705075073357825, \n",
    "                             learning_rate= 0.04681670801808327, n_estimators= 2340, max_depth= 3, \n",
    "                             min_child_weight= 2, num_parallel_tree= 1,\n",
    "                             objective= 'reg:squarederror', random_state=42)\n",
    "\n",
    "lr_model = LinearRegression()\n",
    "\n",
    "br_model = BayesianRidge(alpha_init= 1.4228919418021304, lambda_init= 0.03899454131279402, compute_score=True)\n",
    "\n",
    "lightgbm_model = LGBMRegressor(objective='regression', \n",
    "                       num_leaves=9,\n",
    "                       learning_rate=0.0019200325083996161, \n",
    "                       n_estimators=7000,\n",
    "                       max_bin=200, \n",
    "                       subsample= 0.40495582495561083, \n",
    "                       bagging_fraction=0.4240987934393466,\n",
    "                       bagging_freq=4, \n",
    "                       bagging_seed=8,\n",
    "                       feature_fraction=0.1309662269268009,\n",
    "                       feature_fraction_seed=8,\n",
    "                       min_sum_hessian_in_leaf = 1,\n",
    "                       verbose=-1,\n",
    "                       random_state=42)\n",
    "\n",
    "svr_model = make_pipeline(RobustScaler(), SVR(C=29.994348636600485, epsilon= 0.019482341995527, \n",
    "                                              gamma=0.0009992835981754227))\n",
    "\n",
    "dtr_model = DecisionTreeRegressor(random_state=42)\n",
    "\n",
    "rfr_model = RandomForestRegressor(n_estimators=1200,\n",
    "                          max_depth=10,\n",
    "                          min_weight_fraction_leaf=0,\n",
    "                          min_samples_split=5,\n",
    "                          min_samples_leaf=1,\n",
    "                          max_leaf_nodes=90,\n",
    "                          max_features=None,\n",
    "                          oob_score=True,\n",
    "                          random_state=42)\n",
    "\n",
    "gbr_model = GradientBoostingRegressor(n_estimators=2200,\n",
    "                                learning_rate=0.05005099614548874,\n",
    "                                max_depth=2,\n",
    "                                max_features='sqrt',\n",
    "                                min_samples_leaf=3,\n",
    "                                min_samples_split=24,\n",
    "                                loss='huber',\n",
    "                                random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "851c2b84",
   "metadata": {},
   "source": [
    "To define the scoring metric. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "44fd6059",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse_cv(model,X,y):\n",
    "    rmse = np.sqrt(-cross_val_score(model, X, y, scoring=\"neg_mean_squared_error\", cv=kf))\n",
    "    return rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11dea83e",
   "metadata": {},
   "source": [
    "To define the model names and models and then create a loop to run each model and print out their scores. Unfortunately, there doesn't seem to be an easy way to suppress the output statements from CatBoost nor the warning statements from LGBMRegressor. So there will be a lot of unnecessary verbiage below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "dec2b5b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_names = ['CatBoostRegressor','XGBRegressor','LinearRegression','BayesianRidge','LGBMRegressor',\n",
    "               'SVR','DecisionTreeRegressor','RandomForestRegressor','GradientBoostingRegressor']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e86e3ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [cat_model,xgb_model,lr_model,br_model,lightgbm_model,svr_model,dtr_model,rfr_model,gbr_model]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0547b84b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] feature_fraction is set=0.1309662269268009, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.1309662269268009\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.4240987934393466, subsample=0.40495582495561083 will be ignored. Current value: bagging_fraction=0.4240987934393466\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1309662269268009, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.1309662269268009\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.4240987934393466, subsample=0.40495582495561083 will be ignored. Current value: bagging_fraction=0.4240987934393466\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1309662269268009, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.1309662269268009\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.4240987934393466, subsample=0.40495582495561083 will be ignored. Current value: bagging_fraction=0.4240987934393466\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1309662269268009, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.1309662269268009\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.4240987934393466, subsample=0.40495582495561083 will be ignored. Current value: bagging_fraction=0.4240987934393466\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1309662269268009, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.1309662269268009\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.4240987934393466, subsample=0.40495582495561083 will be ignored. Current value: bagging_fraction=0.4240987934393466\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1309662269268009, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.1309662269268009\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.4240987934393466, subsample=0.40495582495561083 will be ignored. Current value: bagging_fraction=0.4240987934393466\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1309662269268009, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.1309662269268009\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.4240987934393466, subsample=0.40495582495561083 will be ignored. Current value: bagging_fraction=0.4240987934393466\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1309662269268009, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.1309662269268009\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.4240987934393466, subsample=0.40495582495561083 will be ignored. Current value: bagging_fraction=0.4240987934393466\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1309662269268009, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.1309662269268009\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.4240987934393466, subsample=0.40495582495561083 will be ignored. Current value: bagging_fraction=0.4240987934393466\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1309662269268009, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.1309662269268009\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.4240987934393466, subsample=0.40495582495561083 will be ignored. Current value: bagging_fraction=0.4240987934393466\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1309662269268009, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.1309662269268009\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.4240987934393466, subsample=0.40495582495561083 will be ignored. Current value: bagging_fraction=0.4240987934393466\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1309662269268009, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.1309662269268009\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.4240987934393466, subsample=0.40495582495561083 will be ignored. Current value: bagging_fraction=0.4240987934393466\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n"
     ]
    }
   ],
   "source": [
    "score_rmse = []\n",
    "for i in models:\n",
    "    rmse = np.sqrt(-cross_val_score(i, X, y, scoring=\"neg_mean_squared_error\", cv=kf))\n",
    "    ave_rmse = np.mean(rmse)\n",
    "    score_rmse.append(ave_rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9ad2452d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_scores = list(zip(model_names, score_rmse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c16ab10f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('CatBoostRegressor', 0.11669236576542252),\n",
       " ('XGBRegressor', 0.11713185871905746),\n",
       " ('LinearRegression', 0.13032006816252495),\n",
       " ('BayesianRidge', 0.13086931854309383),\n",
       " ('LGBMRegressor', 0.11772110599492115),\n",
       " ('SVR', 0.13006520771776484),\n",
       " ('DecisionTreeRegressor', 0.20708369204073454),\n",
       " ('RandomForestRegressor', 0.13944195125990577),\n",
       " ('GradientBoostingRegressor', 0.11582818229418641)]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acdc334b",
   "metadata": {},
   "source": [
    "To stack up all the models above except CatBoost, optimized using linear regressor. For some reason CatBoost wasn't able to run correctly using the StackingCVRegressor model. The decision tree regressor model was also removed since it had a significantly higher score than the other models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "352417f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "stackCVreg = StackingCVRegressor(regressors=(xgb_model,lr_model,br_model,lightgbm_model,svr_model,\n",
    "                                            rfr_model,gbr_model),\n",
    "                                meta_regressor=lr_model,\n",
    "                                use_features_in_secondary=True,\n",
    "                                random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b8a259a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] feature_fraction is set=0.1309662269268009, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.1309662269268009\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.4240987934393466, subsample=0.40495582495561083 will be ignored. Current value: bagging_fraction=0.4240987934393466\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1309662269268009, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.1309662269268009\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.4240987934393466, subsample=0.40495582495561083 will be ignored. Current value: bagging_fraction=0.4240987934393466\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1309662269268009, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.1309662269268009\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.4240987934393466, subsample=0.40495582495561083 will be ignored. Current value: bagging_fraction=0.4240987934393466\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1309662269268009, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.1309662269268009\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.4240987934393466, subsample=0.40495582495561083 will be ignored. Current value: bagging_fraction=0.4240987934393466\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1309662269268009, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.1309662269268009\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.4240987934393466, subsample=0.40495582495561083 will be ignored. Current value: bagging_fraction=0.4240987934393466\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1309662269268009, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.1309662269268009\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.4240987934393466, subsample=0.40495582495561083 will be ignored. Current value: bagging_fraction=0.4240987934393466\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1309662269268009, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.1309662269268009\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.4240987934393466, subsample=0.40495582495561083 will be ignored. Current value: bagging_fraction=0.4240987934393466\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1309662269268009, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.1309662269268009\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.4240987934393466, subsample=0.40495582495561083 will be ignored. Current value: bagging_fraction=0.4240987934393466\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1309662269268009, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.1309662269268009\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.4240987934393466, subsample=0.40495582495561083 will be ignored. Current value: bagging_fraction=0.4240987934393466\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1309662269268009, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.1309662269268009\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.4240987934393466, subsample=0.40495582495561083 will be ignored. Current value: bagging_fraction=0.4240987934393466\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1309662269268009, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.1309662269268009\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.4240987934393466, subsample=0.40495582495561083 will be ignored. Current value: bagging_fraction=0.4240987934393466\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1309662269268009, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.1309662269268009\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.4240987934393466, subsample=0.40495582495561083 will be ignored. Current value: bagging_fraction=0.4240987934393466\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1309662269268009, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.1309662269268009\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.4240987934393466, subsample=0.40495582495561083 will be ignored. Current value: bagging_fraction=0.4240987934393466\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1309662269268009, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.1309662269268009\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.4240987934393466, subsample=0.40495582495561083 will be ignored. Current value: bagging_fraction=0.4240987934393466\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1309662269268009, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.1309662269268009\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.4240987934393466, subsample=0.40495582495561083 will be ignored. Current value: bagging_fraction=0.4240987934393466\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] feature_fraction is set=0.1309662269268009, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.1309662269268009\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.4240987934393466, subsample=0.40495582495561083 will be ignored. Current value: bagging_fraction=0.4240987934393466\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1309662269268009, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.1309662269268009\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.4240987934393466, subsample=0.40495582495561083 will be ignored. Current value: bagging_fraction=0.4240987934393466\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1309662269268009, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.1309662269268009\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.4240987934393466, subsample=0.40495582495561083 will be ignored. Current value: bagging_fraction=0.4240987934393466\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1309662269268009, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.1309662269268009\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.4240987934393466, subsample=0.40495582495561083 will be ignored. Current value: bagging_fraction=0.4240987934393466\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1309662269268009, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.1309662269268009\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.4240987934393466, subsample=0.40495582495561083 will be ignored. Current value: bagging_fraction=0.4240987934393466\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1309662269268009, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.1309662269268009\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.4240987934393466, subsample=0.40495582495561083 will be ignored. Current value: bagging_fraction=0.4240987934393466\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1309662269268009, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.1309662269268009\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.4240987934393466, subsample=0.40495582495561083 will be ignored. Current value: bagging_fraction=0.4240987934393466\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1309662269268009, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.1309662269268009\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.4240987934393466, subsample=0.40495582495561083 will be ignored. Current value: bagging_fraction=0.4240987934393466\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1309662269268009, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.1309662269268009\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.4240987934393466, subsample=0.40495582495561083 will be ignored. Current value: bagging_fraction=0.4240987934393466\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1309662269268009, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.1309662269268009\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.4240987934393466, subsample=0.40495582495561083 will be ignored. Current value: bagging_fraction=0.4240987934393466\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1309662269268009, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.1309662269268009\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.4240987934393466, subsample=0.40495582495561083 will be ignored. Current value: bagging_fraction=0.4240987934393466\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1309662269268009, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.1309662269268009\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.4240987934393466, subsample=0.40495582495561083 will be ignored. Current value: bagging_fraction=0.4240987934393466\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1309662269268009, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.1309662269268009\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.4240987934393466, subsample=0.40495582495561083 will be ignored. Current value: bagging_fraction=0.4240987934393466\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1309662269268009, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.1309662269268009\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.4240987934393466, subsample=0.40495582495561083 will be ignored. Current value: bagging_fraction=0.4240987934393466\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1309662269268009, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.1309662269268009\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.4240987934393466, subsample=0.40495582495561083 will be ignored. Current value: bagging_fraction=0.4240987934393466\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] feature_fraction is set=0.1309662269268009, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.1309662269268009\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.4240987934393466, subsample=0.40495582495561083 will be ignored. Current value: bagging_fraction=0.4240987934393466\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1309662269268009, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.1309662269268009\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.4240987934393466, subsample=0.40495582495561083 will be ignored. Current value: bagging_fraction=0.4240987934393466\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1309662269268009, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.1309662269268009\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.4240987934393466, subsample=0.40495582495561083 will be ignored. Current value: bagging_fraction=0.4240987934393466\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1309662269268009, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.1309662269268009\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.4240987934393466, subsample=0.40495582495561083 will be ignored. Current value: bagging_fraction=0.4240987934393466\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1309662269268009, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.1309662269268009\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.4240987934393466, subsample=0.40495582495561083 will be ignored. Current value: bagging_fraction=0.4240987934393466\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1309662269268009, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.1309662269268009\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.4240987934393466, subsample=0.40495582495561083 will be ignored. Current value: bagging_fraction=0.4240987934393466\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1309662269268009, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.1309662269268009\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.4240987934393466, subsample=0.40495582495561083 will be ignored. Current value: bagging_fraction=0.4240987934393466\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1309662269268009, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.1309662269268009\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.4240987934393466, subsample=0.40495582495561083 will be ignored. Current value: bagging_fraction=0.4240987934393466\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1309662269268009, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.1309662269268009\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.4240987934393466, subsample=0.40495582495561083 will be ignored. Current value: bagging_fraction=0.4240987934393466\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1309662269268009, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.1309662269268009\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.4240987934393466, subsample=0.40495582495561083 will be ignored. Current value: bagging_fraction=0.4240987934393466\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1309662269268009, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.1309662269268009\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.4240987934393466, subsample=0.40495582495561083 will be ignored. Current value: bagging_fraction=0.4240987934393466\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1309662269268009, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.1309662269268009\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.4240987934393466, subsample=0.40495582495561083 will be ignored. Current value: bagging_fraction=0.4240987934393466\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1309662269268009, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.1309662269268009\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.4240987934393466, subsample=0.40495582495561083 will be ignored. Current value: bagging_fraction=0.4240987934393466\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1309662269268009, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.1309662269268009\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.4240987934393466, subsample=0.40495582495561083 will be ignored. Current value: bagging_fraction=0.4240987934393466\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1309662269268009, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.1309662269268009\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.4240987934393466, subsample=0.40495582495561083 will be ignored. Current value: bagging_fraction=0.4240987934393466\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] feature_fraction is set=0.1309662269268009, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.1309662269268009\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.4240987934393466, subsample=0.40495582495561083 will be ignored. Current value: bagging_fraction=0.4240987934393466\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1309662269268009, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.1309662269268009\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.4240987934393466, subsample=0.40495582495561083 will be ignored. Current value: bagging_fraction=0.4240987934393466\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1309662269268009, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.1309662269268009\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.4240987934393466, subsample=0.40495582495561083 will be ignored. Current value: bagging_fraction=0.4240987934393466\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1309662269268009, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.1309662269268009\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.4240987934393466, subsample=0.40495582495561083 will be ignored. Current value: bagging_fraction=0.4240987934393466\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1309662269268009, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.1309662269268009\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.4240987934393466, subsample=0.40495582495561083 will be ignored. Current value: bagging_fraction=0.4240987934393466\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1309662269268009, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.1309662269268009\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.4240987934393466, subsample=0.40495582495561083 will be ignored. Current value: bagging_fraction=0.4240987934393466\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1309662269268009, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.1309662269268009\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.4240987934393466, subsample=0.40495582495561083 will be ignored. Current value: bagging_fraction=0.4240987934393466\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1309662269268009, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.1309662269268009\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.4240987934393466, subsample=0.40495582495561083 will be ignored. Current value: bagging_fraction=0.4240987934393466\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1309662269268009, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.1309662269268009\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.4240987934393466, subsample=0.40495582495561083 will be ignored. Current value: bagging_fraction=0.4240987934393466\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1309662269268009, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.1309662269268009\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.4240987934393466, subsample=0.40495582495561083 will be ignored. Current value: bagging_fraction=0.4240987934393466\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1309662269268009, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.1309662269268009\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.4240987934393466, subsample=0.40495582495561083 will be ignored. Current value: bagging_fraction=0.4240987934393466\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1309662269268009, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.1309662269268009\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.4240987934393466, subsample=0.40495582495561083 will be ignored. Current value: bagging_fraction=0.4240987934393466\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1309662269268009, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.1309662269268009\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.4240987934393466, subsample=0.40495582495561083 will be ignored. Current value: bagging_fraction=0.4240987934393466\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1309662269268009, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.1309662269268009\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.4240987934393466, subsample=0.40495582495561083 will be ignored. Current value: bagging_fraction=0.4240987934393466\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1309662269268009, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.1309662269268009\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.4240987934393466, subsample=0.40495582495561083 will be ignored. Current value: bagging_fraction=0.4240987934393466\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] feature_fraction is set=0.1309662269268009, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.1309662269268009\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.4240987934393466, subsample=0.40495582495561083 will be ignored. Current value: bagging_fraction=0.4240987934393466\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1309662269268009, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.1309662269268009\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.4240987934393466, subsample=0.40495582495561083 will be ignored. Current value: bagging_fraction=0.4240987934393466\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1309662269268009, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.1309662269268009\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.4240987934393466, subsample=0.40495582495561083 will be ignored. Current value: bagging_fraction=0.4240987934393466\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1309662269268009, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.1309662269268009\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.4240987934393466, subsample=0.40495582495561083 will be ignored. Current value: bagging_fraction=0.4240987934393466\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1309662269268009, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.1309662269268009\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.4240987934393466, subsample=0.40495582495561083 will be ignored. Current value: bagging_fraction=0.4240987934393466\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1309662269268009, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.1309662269268009\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.4240987934393466, subsample=0.40495582495561083 will be ignored. Current value: bagging_fraction=0.4240987934393466\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1309662269268009, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.1309662269268009\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.4240987934393466, subsample=0.40495582495561083 will be ignored. Current value: bagging_fraction=0.4240987934393466\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1309662269268009, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.1309662269268009\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.4240987934393466, subsample=0.40495582495561083 will be ignored. Current value: bagging_fraction=0.4240987934393466\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1309662269268009, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.1309662269268009\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.4240987934393466, subsample=0.40495582495561083 will be ignored. Current value: bagging_fraction=0.4240987934393466\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1309662269268009, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.1309662269268009\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.4240987934393466, subsample=0.40495582495561083 will be ignored. Current value: bagging_fraction=0.4240987934393466\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1309662269268009, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.1309662269268009\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.4240987934393466, subsample=0.40495582495561083 will be ignored. Current value: bagging_fraction=0.4240987934393466\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1309662269268009, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.1309662269268009\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.4240987934393466, subsample=0.40495582495561083 will be ignored. Current value: bagging_fraction=0.4240987934393466\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "0.12314497870629128\n"
     ]
    }
   ],
   "source": [
    "print(rmse_cv(stackCVreg,np.array(X),np.array(y)).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba277087",
   "metadata": {},
   "source": [
    "To fit all the models in preparation for creating an overall blended model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5fe4a876",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] feature_fraction is set=0.1309662269268009, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.1309662269268009\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.4240987934393466, subsample=0.40495582495561083 will be ignored. Current value: bagging_fraction=0.4240987934393466\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n"
     ]
    }
   ],
   "source": [
    "cat_model_all_data = cat_model.fit(X,y)\n",
    "xgb_model_all_data = xgb_model.fit(X,y)\n",
    "lr_model_all_data = lr_model.fit(X,y)\n",
    "br_model_all_data = br_model.fit(X,y)\n",
    "lightgbm_model_all_data = lightgbm_model.fit(X,y)\n",
    "svr_model_all_data = svr_model.fit(X,y)\n",
    "rfr_model_all_data = rfr_model.fit(X,y)\n",
    "gbr_model_all_data = gbr_model.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "68d8193f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] feature_fraction is set=0.1309662269268009, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.1309662269268009\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.4240987934393466, subsample=0.40495582495561083 will be ignored. Current value: bagging_fraction=0.4240987934393466\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1309662269268009, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.1309662269268009\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.4240987934393466, subsample=0.40495582495561083 will be ignored. Current value: bagging_fraction=0.4240987934393466\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1309662269268009, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.1309662269268009\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.4240987934393466, subsample=0.40495582495561083 will be ignored. Current value: bagging_fraction=0.4240987934393466\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1309662269268009, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.1309662269268009\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.4240987934393466, subsample=0.40495582495561083 will be ignored. Current value: bagging_fraction=0.4240987934393466\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n",
      "[LightGBM] [Warning] feature_fraction is set=0.1309662269268009, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.1309662269268009\n",
      "[LightGBM] [Warning] min_sum_hessian_in_leaf is set=1, min_child_weight=0.001 will be ignored. Current value: min_sum_hessian_in_leaf=1\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.4240987934393466, subsample=0.40495582495561083 will be ignored. Current value: bagging_fraction=0.4240987934393466\n",
      "[LightGBM] [Warning] bagging_freq is set=4, subsample_freq=0 will be ignored. Current value: bagging_freq=4\n"
     ]
    }
   ],
   "source": [
    "stackCVreg_model = stackCVreg.fit(np.array(X), np.array(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aab22b95",
   "metadata": {},
   "source": [
    "Many of the Kaggle submissions used this type of blending approach. Using 35% for the StackingCVRegressor produced the lowest Kaggle score. Both 30% and 40% produced higher scores. This blended model had noticeable lower scores than the submissions from any of the other single models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "bad50eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def blended_predictions(X):\n",
    "    random_state=42\n",
    "    return ((.12 * svr_model_all_data.predict(X)) + \\\n",
    "            (.12 * cat_model_all_data.predict(X)) + \\\n",
    "            (.12 * gbr_model_all_data.predict(X)) + \\\n",
    "            (.12 * xgb_model_all_data.predict(X)) + \\\n",
    "            (.12 * lightgbm_model_all_data.predict(X)) + \\\n",
    "            (.05 * rfr_model_all_data.predict(X)) + \\\n",
    "            (0.35 * stackCVreg_model.predict(np.array(X))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "67711e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmsle(y, y_pred):\n",
    "    return np.sqrt(mean_squared_error(y, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "84e4c43e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMLSE score on train data:\n",
      "0.0720280357516632\n"
     ]
    }
   ],
   "source": [
    "blended_score = rmsle(y, blended_predictions(X))\n",
    "print('RMLSE score on train data:')\n",
    "print(blended_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ab20e20",
   "metadata": {},
   "source": [
    "To create the submission dataset and to reset the dependent variable of SalePrice back from the earlier conversion to the log of those values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "eaf27bdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "submit = test[['Id']]\n",
    "submit = submit.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c0e70573",
   "metadata": {},
   "outputs": [],
   "source": [
    "submit_predict = blended_predictions(test)\n",
    "submit_predict = np.exp(submit_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "0342d3fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "submit['SalePrice'] = submit_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "395cc685",
   "metadata": {},
   "outputs": [],
   "source": [
    "submit.to_csv('submit_features_blended.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e90f94b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
